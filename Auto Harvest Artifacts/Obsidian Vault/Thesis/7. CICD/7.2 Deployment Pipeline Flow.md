The Auto Harvest project follows a streamlined deployment process designed for **speed, simplicity, and future scalability**. The pipeline automates releases from code commit to cloud deployment, with safety checks, role separation, and an eye toward multi-environment growth.

---

#### üåø Branch Strategy

The repository follows a **trunk-based development model**, using three main branches:

- **`dev`**: Used for development testing. CI builds Docker images tagged as `dev`, which are then tested locally in a **local Kubernetes cluster**.
    
- **`stage`**: Reserved for future use with a cloud staging namespace or cluster. Currently inactive.
    
- **`main`**: Production branch. All merges to `main` trigger a **full CI/CD pipeline** and deploy directly to the production environment in Google Kubernetes Engine (GKE).
    

All pushes to `main` are **blocked** unless done via a **pull request** that passes a required **code review**, ensuring code stability and team accountability.

---

#### üîÅ Automated Production Deployments

Upon merging to `main`, the GitHub Actions pipeline:

1. Authenticates with Google Cloud
    
2. Builds and pushes Docker images for any affected backend services
    
3. Updates Kubernetes Deployments via `kubectl apply` with the new image tag
    

This process is **fully automated**, enabling rapid iteration while maintaining production integrity. Frontend deployments are currently handled manually or via helper scripts (see Section 7.3), but will eventually follow a similar CI/CD pipeline.

---

#### üì¶ Environments and Namespacing

Currently, the system operates in a **single production environment**. However, the Kubernetes cluster is configured to support **namespaced deployments**, and a `staging` namespace is planned for the near future.

Once scaling begins, the plan is to transition to:

- A **shared staging namespace** within the same cluster
    
- Eventually, a **separate staging cluster** to isolate testing from production
    

---

#### üîô Rollback Strategy

If an issue is discovered post-deployment, rollback is performed manually using the following method:

`kubectl set image deployment/<service-name> <container-name>=<previous-image-digest>`

This allows precise control over which service(s) are reverted, using the SHA-based tags from previous builds stored in Artifact Registry.

In the future, this process will be enhanced with:

- Deployment history logs
    
- Slack or dashboard notifications
    
- A CLI helper or one-click rollback via the mobile admin interface
    

---

#### ‚úÖ Observability and Status

Deployment status is monitored through:

- **GitHub Actions logs**, visible per commit and PR
    
- **Cloud Console dashboards** for GKE and Artifact Registry
    
- Basic `kubectl get pods` / `describe` commands for live health checks
    

Cloud Monitoring integration is in progress, which will allow the team to track deployment health and resource usage over time.